{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7c557d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import openai\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "api_key = ''\n",
    "client = openai.OpenAI(api_key=api_key)\n",
    "\n",
    "KG_file_path = './dataset/knowledge graph of chronic pain.xlsx'\n",
    "file_path = './dataset/AI Data Set with Categories.csv'\n",
    "embedding_save_path = './Embeddings_saved/CP_KG_embeddings'\n",
    "\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    if pd.isna(text):\n",
    "        return ''\n",
    "    text = re.sub(r'\\(.*?\\)', '', text).strip()\n",
    "    text = text.replace('_', ' ')\n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    tokens = word_tokenize(text)\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "kg_data = pd.read_excel(KG_file_path, usecols=['subject', 'relation', 'object'])\n",
    "\n",
    "knowledge_graph = {}\n",
    "for index, row in kg_data.iterrows():\n",
    "    subject = row['subject']\n",
    "    relation = row['relation']\n",
    "    obj = row['object']\n",
    "\n",
    "    if subject not in knowledge_graph:\n",
    "        knowledge_graph[subject] = []\n",
    "    knowledge_graph[subject].append((relation, obj))\n",
    "\n",
    "    if obj not in knowledge_graph:\n",
    "        knowledge_graph[obj] = []\n",
    "    knowledge_graph[obj].append((relation, subject))\n",
    "\n",
    "kg_data['object_preprocessed'] = kg_data.apply(\n",
    "    lambda row: preprocess_text(row['object']) if row['relation'] != 'is_a' else None,\n",
    "    axis=1\n",
    ")\n",
    "symptom_nodes = kg_data['object_preprocessed'].dropna().unique().tolist()\n",
    "\n",
    "def get_symptom_embeddings(symptom_nodes, save_path):\n",
    "    embeddings_path = os.path.join(save_path, 'KG_embeddings.npy')\n",
    "    if os.path.exists(embeddings_path):\n",
    "        print(\"load existing embeddings...\")\n",
    "        return np.load(embeddings_path)\n",
    "    else:\n",
    "        print(\"generate new embeddings...\")\n",
    "        symptom_embeddings = []\n",
    "        for symptom in tqdm(symptom_nodes):\n",
    "            response = client.embeddings.create(\n",
    "                input=symptom,\n",
    "                model=\"text-embedding-3-large\"\n",
    "            )\n",
    "            symptom_embeddings.append(response.data[0].embedding)\n",
    "        np.save(embeddings_path, symptom_embeddings)\n",
    "\n",
    "        return np.array(symptom_embeddings)\n",
    "\n",
    "symptom_embeddings = get_symptom_embeddings(symptom_nodes, embedding_save_path)\n",
    "\n",
    "def find_top_n_similar_symptoms(query, symptom_nodes, symptom_embeddings, n):\n",
    "    if pd.isna(query) or not query:\n",
    "        return []\n",
    "    query_preprocessed = preprocess_text(query)\n",
    "    response = client.embeddings.create(\n",
    "        input=query_preprocessed,\n",
    "        model=\"text-embedding-3-large\"\n",
    "    )\n",
    "    query_embedding = response.data[0].embedding\n",
    "    if not query_embedding:\n",
    "        return []\n",
    "\n",
    "    if len(symptom_embeddings) > len(symptom_nodes):\n",
    "        symptom_embeddings = symptom_embeddings[:len(symptom_nodes)]\n",
    "\n",
    "    similarities = cosine_similarity([query_embedding], symptom_embeddings).flatten()\n",
    "\n",
    "    top_n_symptoms = []\n",
    "    unique_symptoms = set()\n",
    "    top_n_indices = similarities.argsort()[::-1]\n",
    "\n",
    "    for i in top_n_indices:\n",
    "        if similarities[i] > 0.5 and symptom_nodes[i] not in unique_symptoms:\n",
    "            top_n_symptoms.append(symptom_nodes[i])\n",
    "            unique_symptoms.add(symptom_nodes[i])\n",
    "        if len(top_n_symptoms) == n:\n",
    "            break\n",
    "\n",
    "    return top_n_symptoms\n",
    "\n",
    "def compute_shortest_path_length(node1, node2, G):\n",
    "    try:\n",
    "        return nx.shortest_path_length(G, source=node1, target=node2)\n",
    "    except nx.NetworkXNoPath:\n",
    "        return float('inf')\n",
    "\n",
    "categories = [\n",
    "    \"thoracoabdominal_pain_syndromes\",\n",
    "    \"neuropathic_pain_syndromes\",\n",
    "    \"craniofacial_pain_syndromes\",\n",
    "    \"cervical_spine_pain_syndromes\",\n",
    "    \"limb_and_joint_pain_syndromes\",\n",
    "    \"back_pain_syndromes\",\n",
    "    \"lumbar_degenerative_and_stenosis_and_radicular_and_sciatic_syndromes\",\n",
    "    \"generalized_pain_syndromes\",\n",
    "\n",
    "]\n",
    "G = nx.Graph()\n",
    "for node, edges in knowledge_graph.items():\n",
    "    for relation, neighbor in edges:\n",
    "        G.add_edge(node, neighbor, relation=relation)\n",
    "\n",
    "def get_diagnoses_for_symptom(symptom):\n",
    "\n",
    "    diagnoses = []\n",
    "    if symptom in G:\n",
    "        for neighbor in G.neighbors(symptom):\n",
    "            edge_data = G.get_edge_data(neighbor, symptom)\n",
    "            if edge_data and 'relation' in edge_data and edge_data['relation'] != 'is_a':\n",
    "                diagnoses.append(neighbor)\n",
    "    return diagnoses\n",
    "\n",
    "def find_closest_category(top_symptoms, categories,top_n):\n",
    "    if isinstance(top_symptoms, pd.Series) and top_symptoms.empty:\n",
    "        print(\"Warning: top_symptoms is empty.\")\n",
    "        return None\n",
    "    category_votes = {category: 0 for category in categories}\n",
    "    for symptom in top_symptoms:\n",
    "        top_symptoms = list(set(top_symptoms))\n",
    "\n",
    "        # print('symptom: ',symptom)\n",
    "        if symptom not in G:\n",
    "            print(f\"Symptom node not found in graph: {symptom}\")\n",
    "            continue\n",
    "\n",
    "        diagnosis_nodes = get_diagnoses_for_symptom(symptom)\n",
    "        for diagnosis in diagnosis_nodes:\n",
    "\n",
    "            individual_diagnoses = diagnosis.split(',')\n",
    "\n",
    "            for single_diagnosis in individual_diagnoses:\n",
    "                single_diagnosis = single_diagnosis.strip().replace(' ', '_').lower()  # 去掉前后空格\n",
    "                if single_diagnosis not in G:\n",
    "                    print(f\"Diagnosis node not found in graph: {single_diagnosis}\")\n",
    "                    continue\n",
    "\n",
    "                min_distance = float('inf')\n",
    "                closest_category = None\n",
    "\n",
    "                for category in categories:\n",
    "                    if category not in G:\n",
    "                        print(f\"Category node not found in graph: {category}\")\n",
    "                        continue\n",
    "\n",
    "                    try:\n",
    "                        distance = nx.shortest_path_length(G, source=single_diagnosis, target=category)\n",
    "                    except nx.NetworkXNoPath:\n",
    "                        distance = float('inf')\n",
    "\n",
    "                    if distance < min_distance:\n",
    "                        min_distance = distance\n",
    "                        closest_category = category\n",
    "\n",
    "                if closest_category:\n",
    "                    category_votes[closest_category] += 1\n",
    "    print(\"Category votes:\", category_votes)\n",
    "\n",
    "    sorted_categories = sorted(category_votes.items(), key=lambda x: x[1], reverse=True)\n",
    "    top_n_categories = [sorted_categories[i][0] for i in range(top_n)]\n",
    "    return top_n_categories\n",
    "\n",
    "\n",
    "def get_keyinfo_for_category(category, knowledge_graph):\n",
    "    keyinfo_values = []\n",
    "    for node, edges in knowledge_graph.items():\n",
    "        if node == category:\n",
    "            for relation, neighbor in edges:\n",
    "                if relation == \"is_a\" and neighbor in knowledge_graph:\n",
    "                    for rel, obj in knowledge_graph[neighbor]:\n",
    "                        if rel == \"has_keyinfo\":\n",
    "                            keyinfo_values.append(obj)\n",
    "    return keyinfo_values\n",
    "\n",
    "def get_subjects_for_objects(objects, knowledge_graph):\n",
    "    subjects = []\n",
    "    processed_objects = [obj.replace(' ', '_') for obj in objects]\n",
    "    for obj in processed_objects:\n",
    "        for index, row in knowledge_graph.iterrows():\n",
    "            if row['object'] == obj:\n",
    "                subjects.append(row['subject'])\n",
    "    return subjects\n",
    "\n",
    "def find_level3_for_symptoms(top_symptoms, knowledge_graph):\n",
    "    level3_connections = {}\n",
    "    for symptom in top_symptoms:\n",
    "        subjects = get_subjects_for_objects([symptom], knowledge_graph)\n",
    "        for subject in subjects:\n",
    "            if subject in level3_connections:\n",
    "                level3_connections[subject] += 1\n",
    "            else:\n",
    "                level3_connections[subject] = 1\n",
    "    return level3_connections\n",
    "\n",
    "def print_symptom_and_disease(symptom_nodes):\n",
    "    for symptom in symptom_nodes:\n",
    "        subjects = get_subjects_for_objects([symptom], kg_data)\n",
    "\n",
    "\n",
    "def main_get_category_and_level3(n, participant_no,top_n):\n",
    "    data = pd.read_csv(file_path, encoding='ISO-8859-1')\n",
    "\n",
    "    row = data.loc[data['Participant No.'] == str(participant_no)]\n",
    "    if row.empty:\n",
    "        print(f\"Participant No. {participant_no} not found!\")\n",
    "        return None\n",
    "\n",
    "    tr = row[\"Level 2\"].values[0]\n",
    "    tr=tr.split(\",\")[0]\n",
    "\n",
    "    level3real = row[\"Processed Diagnosis\"].values[0]\n",
    "\n",
    "    pain_location = row[\"Pain Presentation and Description\"].values[0]\n",
    "    pain_symptoms = row[\"Pain descriptions and assorted symptoms (self-report)\"].values[0]\n",
    "    pain_restriction = row[\"Pain restriction\"].values[0]\n",
    "    print(f'pain_location: {pain_location}')\n",
    "    print(f'pain_symptoms: {pain_symptoms}')\n",
    "    print(f'pain_restrction: {pain_restriction}')\n",
    "    if pd.isna(pain_location):\n",
    "        pain_location = ''\n",
    "    if pd.isna(pain_symptoms):\n",
    "        pain_symptoms = ''\n",
    "    if pd.isna(pain_restriction):\n",
    "        pain_symptoms = ''\n",
    "\n",
    "\n",
    "    def process_symptom_field(field_value, symptom_nodes, symptom_embeddings, n):\n",
    "        if pd.isna(field_value) or field_value == '':\n",
    "            return []\n",
    "        return find_top_n_similar_symptoms(field_value, symptom_nodes, symptom_embeddings, n)\n",
    "\n",
    "    top_5_location_nodes = process_symptom_field(pain_location, symptom_nodes, symptom_embeddings, n)\n",
    "    top_5_symptom_nodes = process_symptom_field(pain_symptoms, symptom_nodes, symptom_embeddings, n)\n",
    "    top_5_painrestriction_nodes = process_symptom_field(pain_restriction, symptom_nodes, symptom_embeddings, n)\n",
    "\n",
    "\n",
    "    top_5_location_nodes_original = kg_data.loc[kg_data['object_preprocessed'].isin(top_5_location_nodes), 'object'].drop_duplicates()\n",
    "    top_5_symptom_nodes_original = kg_data.loc[kg_data['object_preprocessed'].isin(top_5_symptom_nodes), 'object'].drop_duplicates()\n",
    "    top_5_painrestriction_original = kg_data.loc[kg_data['object_preprocessed'].isin(top_5_painrestriction_nodes), 'object'].drop_duplicates()\n",
    "\n",
    "\n",
    "    most_similar_category = find_closest_category(\n",
    "        list(top_5_location_nodes_original) + list(top_5_symptom_nodes_original)+ list(top_5_painrestriction_original),\n",
    "        categories,\n",
    "        top_n\n",
    "    )\n",
    "    return most_similar_category\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca0e3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import faiss\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from huggingface_hub import InferenceClient\n",
    "from KG_Retrieve import main_get_category_and_level3\n",
    "from authentication import api_key,hf_token\n",
    "\n",
    "client = openai.OpenAI(api_key=api_key)\n",
    "\n",
    "def get_embeddings(texts):\n",
    "    embeddings = []\n",
    "    for text in tqdm(texts):\n",
    "        response = client.embeddings.create(\n",
    "            input=text,\n",
    "            model=\"text-embedding-3-large\"\n",
    "        )\n",
    "        embeddings.append(response.data[0].embedding)\n",
    "    return np.array(embeddings)\n",
    "\n",
    "\n",
    "def get_query_embedding(query):\n",
    "    return get_embeddings([query])[0]\n",
    "\n",
    "\n",
    "# FAISS\n",
    "def Faiss(document_embeddings, query_embedding, k):\n",
    "    # index = faiss.IndexFlatL2(document_embeddings.shape[1])\n",
    "    index = faiss.IndexFlatIP(document_embeddings.shape[1])\n",
    "    # index = faiss.IndexHNSWFlat(document_embeddings.shape[1])\n",
    "    index.add(document_embeddings)\n",
    "    _, indices = index.search(np.array([query_embedding]), k)\n",
    "    print(\"index: \", indices)\n",
    "    return indices\n",
    "\n",
    "def extract_diagnosis(generated_text):\n",
    "    diagnoses = re.findall(r'\\*\\*Diagnosis\\*\\*:\\s(.*?)\\n', generated_text)\n",
    "    return diagnoses\n",
    "\n",
    "def remove_parentheses(text):\n",
    "    return re.sub(r'\\(.*?\\)', '', text).strip()\n",
    "\n",
    "def KG_preprocess(file_path):\n",
    "    kg_data = pd.read_excel(file_path, usecols=['subject', 'relation', 'object'])\n",
    "    kg_data['subject'] = kg_data['subject'].apply(remove_parentheses)\n",
    "    kg_data['object'] = kg_data['object'].apply(remove_parentheses)\n",
    "\n",
    "    knowledge_graph = {}\n",
    "    for index, row in kg_data.iterrows():\n",
    "        subject = row['subject']\n",
    "        relation = row['relation']\n",
    "        obj = row['object']\n",
    "\n",
    "        if subject not in knowledge_graph:\n",
    "            knowledge_graph[subject] = []\n",
    "        knowledge_graph[subject].append((relation, obj))\n",
    "\n",
    "        if obj not in knowledge_graph:\n",
    "            knowledge_graph[obj] = []\n",
    "        knowledge_graph[obj].append((relation, subject))\n",
    "    return knowledge_graph\n",
    "\n",
    "\n",
    "def extract_features_from_json(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        patient_case = json.load(file)\n",
    "\n",
    "    pain_location = patient_case.get(\"Pain Presentation and Description Areas of pain as per physiotherapy input\", \"\")\n",
    "    pain_symptoms = patient_case.get(\n",
    "        \"Pain descriptions and assorted symptoms (self-report) Associated symptoms include: parasthesia, numbness, weakness, tingling, pins and needles\",\n",
    "        \"\")\n",
    "\n",
    "    return pain_location, pain_symptoms\n",
    "\n",
    "level_3_to_level_2 = {\n",
    "    # Here are subcategories: diseases\n",
    "    # Examples: \n",
    "    \n",
    "    # Respiratory System\n",
    "    \"acute_copd_exacerbation_infection\": \"respiratory_system\",\n",
    "\n",
    "    # Cardiovascular System\n",
    "    \"atrial_fibrillation\": \"cardiovascular_system\",\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "def get_additional_info_from_level_2(participant_no,  kg_path,top_n,match_n):\n",
    "    level_2_values=main_get_category_and_level3(match_n,participant_no,top_n)\n",
    "    additional_info = []\n",
    "    if not level_2_values:\n",
    "        print(f\"No data found for Participant No.: {participant_no}\")\n",
    "        return None\n",
    "    for level_2_value in level_2_values:\n",
    "        relevant_level_3_descriptions = [desc for desc, level2 in level_3_to_level_2.items() if level2 == level_2_value]\n",
    "        print(\"Relevant Level 3 Descriptions:\", relevant_level_3_descriptions)\n",
    "        if not relevant_level_3_descriptions:\n",
    "            print(\"No Level 3 descriptions found for Level 2:\", level_2_value)\n",
    "            continue\n",
    "\n",
    "        kg_data = pd.read_excel(kg_path, usecols=['subject', 'relation', 'object'])\n",
    "        if kg_data.empty:\n",
    "            print(\"Knowledge graph data is empty.\")\n",
    "            return None\n",
    "\n",
    "        merged_info = {}\n",
    "\n",
    "        for level_3 in relevant_level_3_descriptions:\n",
    "            related_info = kg_data[kg_data['subject'] == level_3]\n",
    "\n",
    "            if related_info.empty:\n",
    "                print(f\"No related information found in KG for: {level_3}\")\n",
    "            else:\n",
    "                for _, row in related_info.iterrows():\n",
    "                    subject = row['subject']\n",
    "                    relation = row['relation'].replace('_', ' ')\n",
    "                    obj = row['object']\n",
    "\n",
    "                    if (subject, relation) in merged_info:\n",
    "                        merged_info[(subject, relation)].append(obj)\n",
    "                    else:\n",
    "                        merged_info[(subject, relation)] = [obj]\n",
    "\n",
    "        # K\n",
    "        additional_info = []\n",
    "        for (subject, relation), objects in merged_info.items():\n",
    "            sentence = f\"{subject} {relation} {', '.join(objects)}\"\n",
    "            additional_info.append(sentence)\n",
    "\n",
    "    if not additional_info:\n",
    "        print(\"No additional information found.\")\n",
    "        return None\n",
    "\n",
    "    final_info = ', '.join(additional_info)\n",
    "    print(\"Additional Info:\", final_info)\n",
    "    return final_info\n",
    "\n",
    "\n",
    "def get_system_prompt_for_RAGKG():\n",
    "    return '''\n",
    "        You are a knowledgeable medical assistant with expertise in pain management.\n",
    "        Your tasks are:\n",
    "        1. Analyse and refer to the retrieved similar patients' cases and knowledge graph which may be relevant to the diagnosis and assist with new patient cases.\n",
    "2. Output of \"Diagnoses\" must come from : acute copd exacerbation infection, bronchiectasis, bronchiolitis, bronchitis, bronchospasm acute asthma exacerbation, pulmonary embolism, pulmonary neoplasm, spontaneous pneumothorax, urti, viral pharyngitis, whooping cough, acute laryngitis, acute pulmonary edema, croup, larygospasm, epiglottitis, pneumonia, atrial fibrillation, myocarditis, pericarditis, psvt, possible nstemi stemi, stable angina, unstable angina, gerd, boerhaave syndrome, pancreatic neoplasm, scombroid food poisoning, inguinal hernia, tuberculosis, hiv initial infection, ebola, influenza, chagas, acute otitis media, acute rhinosinusitis, allergic sinusitis, chronic rhinosinusitis, myasthenia gravis, guillain barre syndrome, cluster headache, acute dystonic reactions, sle, sarcoidosis, anaphylaxis, panic attack, spontaneous rib fracture, anemia.        3. You are given differences of diagnoses of similar symptoms or pain locations. Read that information as a reference to your diagnostic if applicable.\n",
    "        4. Do mind the nuance between these factors of similar diagnosis with knowledge graph information and consider it when diagnose new patient's informtation.\n",
    "        5. Ensure that the recommendations are evidence-based and consider the most recent and effective practices in pain management.\n",
    "        6. The output should include four specific treatment-related fields:\n",
    "           - \"Diagnoses (related to pain)\"\n",
    "           - Explanations of diagnose\n",
    "           - \"Pain/General Physiotherapist Treatments\\nSession No.: General Overview\\n- Specific interventions/treatments\"\n",
    "           - \"Pain Psychologist Treatments\"\n",
    "           - \"Pain Medicine Treatments\"\n",
    "        7. In \"Diagnoses\", only output the diagnosis itself. Place all other explanations and analyses (if any) into \"Explanations of diagnose\".\n",
    "        8. You can leave Psychologist Treatments blank if not applicable for the case, leaving text \"Not applicable\"\n",
    "        9.If you think information is needed, guide the doctor to ask further questions which following areas to distinguish between the most likely diseases: Pain restriction; Location; Symptom. Seperate answers with \",\". The output should only include aspects.\n",
    "        10. The output should follow this structured format:\n",
    "        \n",
    "\n",
    "    ### Diagnoses\n",
    "    1. **Diagnosis**: Answer.\n",
    "    2. **Explanations of diagnose**: Answer.\n",
    "    \n",
    "    ### Instructive question\n",
    "    1. **Questions**: Answer.\n",
    "    \n",
    "    ### Pain/General Physiotherapist Treatments\n",
    "    1. **Session No.: General Overview**\n",
    "        - **Specific interventions/treatments**:\n",
    "        - **Goals**:\n",
    "        - **Exercises**:\n",
    "        - **Manual Therapy**:\n",
    "        - **Techniques**:\n",
    "\n",
    "    2. **Exercise Recommendations from the Exercise List**:\n",
    "\n",
    "    ### Pain Psychologist Treatments(if applicable)\n",
    "    1. **Treatment 1**: \n",
    "    \n",
    "    ### Pain Medicine Treatments\n",
    "\n",
    "\n",
    "    ### Recommendations for Further Evaluations\n",
    "    1. **Evaluation 1**:\n",
    "    '''\n",
    "\n",
    "\n",
    "def generate_diagnosis_report(path, query, retrieved_documents, i,top_n,match_n,model):\n",
    "    system_prompt_RAGKG = get_system_prompt_for_RAGKG()\n",
    "    system_prompt=system_prompt_RAGKG\n",
    "    additional_info= get_additional_info_from_level_2(i ,path,top_n=top_n,match_n=match_n)\n",
    "\n",
    "    prompt = f\"{query}\\nRetrieved Documents: {retrieved_documents}\\nInformation from knowledge graph about relevant diagnoses, if you think the patient's disease is relevant from the suggestions provided by the atlas please refer to thoses details to distinguish similar diagnoses : {additional_info} .Now complete the tasks in that format\"\n",
    "\n",
    "\n",
    "    ############################################################################################openai\n",
    "    if model =='gpt-4o' or 'gpt-4o-mini' or 'gpt-3.5-turbo-0125':\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    else:\n",
    "        prompt=f\"\"\"<s>[INST] <<SYS>> {system_prompt} <</SYS>> {prompt} [/INST]\"\"\"\n",
    "        LLMclient = InferenceClient(\n",
    "            \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
    "            # \"meta-llama/Llama-2-13b-chat-hf\",\n",
    "            # \"meta-llama/Meta-Llama-3.1-70B-Instruct\",\n",
    "            # \"meta-llama/Llama-2-13b-hf\",\n",
    "            # \"Qwen/Qwen2-7B-Instruct\",\n",
    "            # \"Qwen/Qwen2.5-0.5B-Instruct\",\n",
    "            # \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "            # 'mistralai/Mixtral-8x7B-Instruct-v0.1',\n",
    "            token=hf_token\n",
    "        )\n",
    "        response = LLMclient.text_generation(prompt=prompt,max_new_tokens=400)\n",
    "        return response\n",
    "\n",
    "def save_results_to_csv(results, output_file):\n",
    "    df = pd.DataFrame(results,\n",
    "                      columns=['Participant No.', 'Generated Diagnosis', 'True Diagnosis', 'Original Diagnosis'])\n",
    "    df.to_csv(output_file, index=False)\n",
    "\n",
    "\n",
    "folder_path=\".dataset/df/train\"\n",
    "documents = [os.path.join(folder_path, file_name) for file_name in os.listdir(folder_path) if\n",
    "             os.path.isfile(os.path.join(folder_path, file_name))]\n",
    "\n",
    "document_embeddings_file_path='./dataset/document_embeddings.npy'\n",
    "\n",
    "def save_embeddings(embeddings, file_path):\n",
    "    np.save(file_path, embeddings)\n",
    "\n",
    "def load_embeddings(file_path):\n",
    "    return np.load(file_path)\n",
    "if os.path.exists(document_embeddings_file_path):\n",
    "    document_embeddings = load_embeddings(document_embeddings_file_path)\n",
    "else:\n",
    "    document_embeddings = get_embeddings(documents)\n",
    "    save_embeddings(document_embeddings, document_embeddings_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a2bed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from main_MedRAG import get_query_embedding, Faiss,  extract_diagnosis, documents, document_embeddings,generate_diagnosis_report, save_results_to_csv, get_additional_info_from_level_2,KG_preprocess, get_embeddings\n",
    "from authentication import ob_path,test_folder_path,ground_truth_file_path,augmented_features_path\n",
    "\n",
    "disease_list = [\n",
    "    \"Head pain\", \"Migraine\", \"Trigeminal neuralgia\", \"Cervical spondylosis\", \"Chronic neck pain\", \"Neck pain\",\n",
    "    \"Chest pain\", \"Abdominal pain\", \"Limb pain\", \"Shoulder pain\", \"Hip pain\", \"Knee pain\", \"Buttock pain\",\n",
    "    \"Calf pain\", \"Low back pain\", \"Chronic low back pain\", \"Mechanical low back pain\", \"Upper back pain\",\n",
    "    \"Degenerative disc disease\", \"Lumbar spondylosis\", \"Lumbar canal stenosis\", \"Spinal stenosis\", \"Foraminal stenosis\",\n",
    "    \"Lumbar_radicular_pain\", \"Radicular pain\", \"Sciatica\", \"Lumbosacral pain\", \"Generalized body pain\", \"Fibromyalgia\",\n",
    "    \"Musculoskeletal pain\", \"Myofascial pain syndrome\", \"Neuropathic pain\", \"Post-herpetic neuralgia\"\n",
    "]\n",
    "ground_truth = pd.read_csv(ground_truth_file_path, header=0)\n",
    "\n",
    "results = []\n",
    "file_paths = os.listdir(test_folder_path)\n",
    "topk=1\n",
    "top_n=1\n",
    "match_n=5\n",
    "samplerange=range(1,552)\n",
    "\n",
    "for i in tqdm(samplerange):\n",
    "\n",
    "    print(\"topk:\",topk)\n",
    "    print(\"top_ns:\",top_n)\n",
    "    print(\"match_n:\", match_n)\n",
    "    print(\"i= \",i)\n",
    "    file_path = os.path.join(test_folder_path, f\"participant_{i}.json\")\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f'{i} is not found')\n",
    "        continue\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        new_patient_case = json.load(file)\n",
    "        print(new_patient_case)\n",
    "\n",
    "    participant_no = new_patient_case['Participant No.']\n",
    "    query = json.dumps(new_patient_case)\n",
    "\n",
    "    success = False\n",
    "    while not success:\n",
    "        try:\n",
    "            query_embedding = get_query_embedding(query)\n",
    "            indices = Faiss(document_embeddings, query_embedding,k=topk)\n",
    "            retrieved_documents = [documents[i] for i in indices[0]]\n",
    "            final_retrieved_info =[]\n",
    "            correct_count = 0\n",
    "            for retrieved_document in retrieved_documents:\n",
    "                with open(retrieved_document, 'r') as file:\n",
    "                    patient_case = json.load(file)\n",
    "                    patient_case_json = json.dumps(patient_case)\n",
    "                    patient_case_dict = json.loads(patient_case_json)\n",
    "                    filtered_patient_case_dict = {\n",
    "                        key: patient_case_dict[key] for key in [\n",
    "                            \"Processed Diagnosis\",\n",
    "                            \"Pain Presentation and Description Areas of pain as per physiotherapy input\",\n",
    "                            \"Pain descriptions and assorted symptoms (self-report) Associated symptoms include: parasthesia, numbness, weakness, tingling, pins and needles\",\n",
    "                            \"Pain/General Physiotherapist Treatments (Treatments\\nSession No.: General Overview\\n- Specific interventions/treatments)\",\n",
    "                            \"Pain Psychologist Treatments (Treatments)\",\n",
    "                            \"Pain Medicine Treatments (Treatments)\",\n",
    "                        ] if key in patient_case_dict\n",
    "                    }\n",
    "                    final_retrieved_info.append(filtered_patient_case_dict)\n",
    "\n",
    "    # ——————————————————————————————————————————————————————————————————————————————————\n",
    "            true_diagnosis_row = ground_truth.loc[ground_truth['Participant No.'] == participant_no]\n",
    "            if true_diagnosis_row.empty:\n",
    "                print(f\"True diagnosis for patient_{participant_no} not found in ground truth data\")\n",
    "                break\n",
    "\n",
    "            true_diagnosis = true_diagnosis_row['Processed Diagnosis'].values[0]\n",
    "            ori_truth = true_diagnosis_row['Diagnoses (related to pain)'].values[0]\n",
    "            generated_report_ori = generate_diagnosis_report(augmented_features_path,query, final_retrieved_info, i,top_n=top_n,match_n=match_n)\n",
    "            print(generated_report_ori)\n",
    "\n",
    "            generated_diagnosis = re.findall(r'\\*\\*Diagnosis\\*\\*:\\s*(.*?)(?:\\.|\\n|$)', generated_report_ori)\n",
    "            if not generated_diagnosis:\n",
    "                print(\"Generated diagnosis is either empty or not in the specified disease list. Retrying...\")\n",
    "                results.append([participant_no, '', true_diagnosis, ori_truth, generated_report_ori])\n",
    "                break\n",
    "            else:\n",
    "                print(\"Success!!!\")\n",
    "\n",
    "\n",
    "            results.append([participant_no, generated_diagnosis[0], true_diagnosis, ori_truth,generated_report_ori])\n",
    "            success = True\n",
    "            print('________________________________________________________________')\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing patient_{participant_no}: {e}. \")\n",
    "\n",
    "output_file = f\"./test_results_topk{topk}_topn{top_n}_matchn{match_n}_{samplerange}_MedRAG.csv\"\n",
    "\n",
    "df = pd.DataFrame(results, columns=['Participant No.', 'Generated Diagnosis', 'True Diagnosis', 'Ori Truth','Generated report'])\n",
    "df.to_csv(output_file, index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
